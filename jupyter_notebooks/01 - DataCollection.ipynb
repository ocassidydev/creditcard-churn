{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from kaggle and save raw data in repo.\n",
        "* Inspect the data and save in outputs/datasets/collection.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Kaggle JSON file - the account authentication token. \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Dataset: output/datasets/collection/.csv\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* Were this a real-world project, the inputs/datasets/raw outputs/datasets/ director would ideally be included in .gitignore in order to not push the customer data to the public. For the purposes of project evaluation and the jupyter notebooks running properly for the examiner this will not be done.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Need to change working directory from the current **jupyter_notebooks** folder to the parent folder in order to access the whole project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/creditcard-churn/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/creditcard-churn'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Install required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installation of dependencies listed in requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.18.5\n",
            "  Using cached numpy-1.18.5.zip (5.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pandas==1.4.2\n",
            "  Using cached pandas-1.4.2.tar.gz (4.9 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting matplotlib==3.3.1\n",
            "  Using cached matplotlib-3.3.1.tar.gz (38.8 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting seaborn==0.11.0\n",
            "  Using cached seaborn-0.11.0-py3-none-any.whl (283 kB)\n",
            "Collecting pandas-profiling==3.1.0\n",
            "  Using cached pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB)\n",
            "Collecting plotly==4.12.0\n",
            "  Using cached plotly-4.12.0-py2.py3-none-any.whl (13.1 MB)\n",
            "Collecting ppscore==1.2.0\n",
            "  Using cached ppscore-1.2.0.tar.gz (47 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting altair==4.2.2\n",
            "  Using cached altair-4.2.2-py3-none-any.whl (813 kB)\n",
            "Collecting streamlit==0.85.0\n",
            "  Using cached streamlit-0.85.0-py2.py3-none-any.whl (7.9 MB)\n",
            "Collecting feature-engine==1.0.2\n",
            "  Using cached feature_engine-1.0.2-py2.py3-none-any.whl (152 kB)\n",
            "Collecting imbalanced-learn==0.8.0\n",
            "  Using cached imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Using cached scikit-learn-0.24.2.tar.gz (7.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting xgboost==1.2.1\n",
            "  Using cached xgboost-1.2.1-py3-none-manylinux2010_x86_64.whl (148.9 MB)\n",
            "Collecting yellowbrick==1.3\n",
            "  Using cached yellowbrick-1.3-py3-none-any.whl (271 kB)\n",
            "Collecting Jinja2==3.1.1\n",
            "  Using cached Jinja2-3.1.1-py3-none-any.whl (132 kB)\n",
            "Collecting MarkupSafe==2.0.1\n",
            "  Using cached MarkupSafe-2.0.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting protobuf==3.20\n",
            "  Using cached protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
            "Collecting ipywidgets==8.0.2\n",
            "  Using cached ipywidgets-8.0.2-py3-none-any.whl (134 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from pandas==1.4.2->-r requirements.txt (line 3)) (2.8.2)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r requirements.txt (line 3) and numpy==1.18.5 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested numpy==1.18.5\n",
            "    pandas 1.4.2 depends on numpy>=1.21.0; python_version >= \"3.10\"\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Fetch data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install kaggle package to fetch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle==1.5.12\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from kaggle==1.5.12) (1.16.0)\n",
            "Requirement already satisfied: certifi in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from kaggle==1.5.12) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from kaggle==1.5.12) (2.8.2)\n",
            "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from kaggle==1.5.12) (2.28.2)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-slugify\n",
            "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: urllib3 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from kaggle==1.5.12) (1.26.15)\n",
            "Collecting text-unidecode>=1.3\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests->kaggle==1.5.12) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.11.1/lib/python3.11/site-packages (from requests->kaggle==1.5.12) (3.4)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73031 sha256=31570c46262aba53b66ad18e4cddd3ee62078a882288ca6fc1bad9ffe737b4e7\n",
            "  Stored in directory: /workspace/.pyenv_mirror/pip_cache/wheels/2e/27/39/f44e52756a6407b444143f233abe9fda0e18a23e8b20e0cd1c\n",
            "Successfully built kaggle\n",
            "Installing collected packages: text-unidecode, tqdm, python-slugify, kaggle\n",
            "Successfully installed kaggle-1.5.12 python-slugify-8.0.1 text-unidecode-1.3 tqdm-4.65.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: in order to run this, you must upload your own kaggle.json file to the workspace to authenticate the request. This cell sets the KAGGLE_CONFIG_DIR to the project's directory, and then sets the 'read' permission on kaggle.json to anyone. This is so the kaggle API request can function correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the kaggle dataset path, make a directory for it, and then run a kaggle command to download the dataset to this directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading employee-attrition.zip to inputs/datasets/raw\n",
            "  0%|                                               | 0.00/50.1k [00:00<?, ?B/s]\n",
            "100%|██████████████████████████████████████| 50.1k/50.1k [00:00<00:00, 2.04MB/s]\n"
          ]
        }
      ],
      "source": [
        "KaggleDatasetPath = \"patelprashant/employee-attrition\"\n",
        "DestinationFolder = \"inputs/datasets/raw\"\n",
        "if not os.path.isdir(DestinationFolder):\n",
        "    os.makedirs(DestinationFolder)\n",
        "\n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unzip the downloaded file, delete the zip file and delete the kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  inputs/datasets/raw/employee-attrition.zip\n",
            "  inflating: inputs/datasets/raw/WA_Fn-UseC_-HR-Employee-Attrition.csv  \n"
          ]
        }
      ],
      "source": [
        "! unzip {DestinationFolder}/*.zip -d {DestinationFolder} && rm {DestinationFolder}/*.zip && rm kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename .csv file to a more readable name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "! mv {DestinationFolder}/*Employee-Attrition.csv {DestinationFolder}/EmployeeAttrition.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load and Inspect Kaggle data #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs/datasets/raw/EmployeeAttrition.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(f\"inputs/datasets/raw/EmployeeAttrition.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DataFrame summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check if there are any duplicate customers by inspecting for duplicates in `CLIENTNUM`. There are no duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[df.duplicated(subset=['CLIENTNUM'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change `Attrition_Flag` from categorical variable ('Existing Customer', 'Attrited Customer') to an integer (0 or 1) for use in model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Attrition_Flag'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Attrition_Flag'] = df['Attrition_Flag'].replace({'Attrited Customer': 1, 'Existing Customer': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the `Attrition_Flag` data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Attrition_Flag'].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In case you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(name='outputs/datasets/collection') # create outputs/datasets/collection folder\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "df.to_csv(f\"outputs/datasets/collection/BankChurners.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.1 64-bit ('shims')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

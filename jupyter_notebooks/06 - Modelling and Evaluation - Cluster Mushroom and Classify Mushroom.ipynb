{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar data\n",
        "* Understand the profile for each cluster\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/mushrooms.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Most important features to define a cluster plot\n",
        "* Clusters Profile Description\n",
        "* Cluster Silhouette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Need to change working directory from the current **jupyter_notebooks** folder to the parent folder in order to access the whole project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/collection/mushrooms.csv\")\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cluster Mushroom Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As previously identified in the Data Cleaning notebook, no data cleaning is needed. Target Encoding followed by Smart Correlated Selection will be used as before. Principal Component Analysis will then been performed to extract correlated components that explain the variance in the data. K-means clustering will be applied to the principal components to identify a set number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Feature Engineering\n",
        "from category_encoders import TargetEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ML algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "cols = df.columns[df.dtypes=='object'].to_list()\n",
        "\n",
        "cat_list=[]\n",
        "\n",
        "for col in cols:\n",
        "    cat_list.append(list(df[col].unique()))\n",
        "\n",
        "def PipelineCluster():\n",
        "    \"\"\" \n",
        "    Creates a pipeline for a k-means clustering model \n",
        "    Taken from Walkthrough Project 02 - churnometer\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        pipeline_base: a sklearn pipeline object, containing the feature engineering, \n",
        "                    scaling, PCA, and modelling steps for the clustering model\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "    ])\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df, df['edible'])\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply PCA separately to the scaled data to determine how many principal components account for the variance in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "n_components = 17\n",
        "\n",
        "def pca_components_analysis(df_pca, n_components):\n",
        "    \"\"\"\n",
        "    Performs principal component analysis on the dataset for a set number of components, \n",
        "    then displays the proportion of the variance in the dataset accounted for in each \n",
        "    Taken from Walkthrough Project 02 - Churnometer\n",
        "\n",
        "    Args:\n",
        "        df_pca: a feature engineered dataframe to perform PCA on\n",
        "        n_components: the number of principal components to create\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=n_components).fit(df_pca)\n",
        "    x_PCA = pca.transform(df_pca)\n",
        "\n",
        "    ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "    dfExplVarRatio = pd.DataFrame(\n",
        "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "        index=ComponentsList,\n",
        "        columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum()\n",
        "\n",
        "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "    print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(np.arange(0, 110, 10))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to reduce the complexity of the model, only the most relevant components which explain the majority of the variance in the dataset will be used. For this, we will look at taking the 6 most important components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components = 6\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will set `n_components` argument of PCA to 6. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    \"\"\" \n",
        "    Creates a pipeline for a k-means clustering model \n",
        "    Taken from Walkthrough Project 02 - churnometer\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        pipeline_base: a sklearn pipeline object, containing the feature engineering, \n",
        "                    scaling, PCA, and modelling steps for the clustering model\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=6, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elbow Method and Silhouette Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the new version of the pipeline, and then fit and transform the data to the feature engineering and PCA steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df, df['edible'])\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will perform the elbow method to determine the best value of k for the k-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,14)) \n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The elbow appears at k=6. We will now visualize the change in silhouette score for differing values of k distributed around this value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "n_cluster_start, n_cluster_stop = 2, 11\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(n_cluster_start, n_cluster_stop), metric='silhouette')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0), colors='yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    plt.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The elbow on the average silhouette score plot is at k=7. To optimize for both average distortion store and average silhouette score, we will choose 7 clusters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit Cluster Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define cluster pipeline based on findings of above sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    \"\"\" \n",
        "    Creates a pipeline for a k-means clustering model \n",
        "    Taken from Walkthrough Project 02 - churnometer\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        pipeline_base: a sklearn pipeline object, containing the feature engineering, \n",
        "                    scaling, PCA, and modelling steps for the clustering model\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"PCA\", PCA(n_components=6, random_state=0)),\n",
        "        (\"model\", KMeans(n_clusters=7, random_state=0))])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy of data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit Cluster pipeline to data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X, X['edible'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add a column `Clusters` from the cluster pipeline predictions to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1], hue=X['Clusters'], style=X['edible'], palette='Set1', alpha=0.6)\n",
        "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], y=pipeline_cluster['model'].cluster_centers_[:, 1], marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
        "plt.xlabel(\"PCA Component 0\")\n",
        "plt.ylabel(\"PCA Component 1\")\n",
        "plt.title(\"PCA Components colored by Clusters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation: We cross check with metrics defined in the ML business case\n",
        "\n",
        "* A minimum mean silhouette score of 0.5 on the model's clusters\n",
        "\n",
        "Getting mean silhouette score and distortion score for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "sample_silhouette_values = silhouette_samples(df_analysis, X['Clusters'])\n",
        "num_clusters = 7\n",
        "cluster_labels = pipeline_cluster['model'].labels_\n",
        "\n",
        "means_lst = []\n",
        "for label in range(num_clusters):\n",
        "    means_lst.append(sample_silhouette_values[cluster_labels == label].mean())\n",
        "\n",
        "np.mean(means_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster['model'].inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appears from the plot of PCA components, cluster and edibility above that the clustering model is somewhat successful at identifying unique groupings of mushrooms, which appear distinct in the rates of edibility among them. The model also has an average silhouette score of 0.52, which satisfies the business requirement for this model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save the cluster predictions from this pipeline to use in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classify Mushroom Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We copy `X` to a DataFrame `dl_clf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters', 'edible'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create classifier pipeline steps. As with the Predict Edible pipeline, this pipeline will use Target Encoding, Smart Correlated Selection, Standard Scaling, Feature Selection, and then will be fitted using an Adaptive Boost Classifier algorithm with the same optimal hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "    \"\"\" \n",
        "    Creates a pipeline for the mushroom classifier\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        pipeline_base: a sklearn pipeline object, containing the feature engineering, \n",
        "                    scaling, PCA, and modelling steps for the classifier model\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetCategoricalEncoder\", TargetEncoder()),\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feat_selection\", SelectFromModel(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), learning_rate=0.01, n_estimators=500))),\n",
        "        (\"model\", AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), learning_rate=0.01, n_estimators=500))\n",
        "    ])\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit classifier to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation: We cross check with metrics defined in the ML business case\n",
        "\n",
        "* A minimum average recall of 0.7 in predicting the model's cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model performs to the required level of precision on the test set, and can therefore be said to be usable for the business case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Most Important Features that Define a Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variables that were selected for in the Feature Selection step of the pipeline are picked out and plotted in terms of feature importance in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_cleaning_feat_eng_steps = 2\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps]).transform(X_train).columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False))\n",
        "\n",
        "# reassign best features in importance order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will store the best_features to use at a later stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load function that plots a table with description for all clusters - the percentages of each category that composes a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DescriptionAllClusters(df):\n",
        "    \"\"\"  \n",
        "    Creates a dataframe describing the distribution of categories within the clusters \n",
        "    Taken from Walkthrough Project 02 - Churnometer\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the dataset\n",
        "\n",
        "    Returns:\n",
        "        DescriptionAllClusters: DataFrame containing the composition of the most important\n",
        "                                variable categories for each cluster\n",
        "    \"\"\"\n",
        "    DescriptionAllClusters = pd.DataFrame(columns=df.drop(['Clusters'], axis=1).columns)\n",
        "\n",
        "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "        EDA_ClusterSubset = df.query(f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
        "        ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset, cluster)\n",
        "        DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
        "\n",
        "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
        "    return DescriptionAllClusters\n",
        "\n",
        "\n",
        "def Clusters_IndividualDescription(EDA_Cluster, cluster):\n",
        "    \"\"\" Produces statistical information on the distribution of variable categories in each cluster \"\"\"\n",
        "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "\n",
        "    for col in EDA_Cluster.columns:\n",
        "        try:\n",
        "            top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "            Description = ''\n",
        "\n",
        "            for x in range(len(top_frequencies)):\n",
        "                freq = top_frequencies.iloc[x]\n",
        "                category = top_frequencies.index[x][0]\n",
        "                CategoryPercentage = int(round(freq*100, 0))\n",
        "                statement = f\"'{category}': {CategoryPercentage}% , \"\n",
        "                Description = Description + statement\n",
        "\n",
        "            ClustersDescription.at[0, col] = Description[:-2]\n",
        "            \n",
        "        except Exception as e:\n",
        "            ClustersDescription.at[0, col] = 'Not available'\n",
        "            print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "\n",
        "    ClustersDescription['Cluster'] = str(cluster)\n",
        "\n",
        "    return ClustersDescription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a custom function to plot cluster distribution per Variable (absolute and relative levels) - this will be used plot the distribution of edibility among clusters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "\n",
        "def cluster_distribution_per_variable(df, target):\n",
        "    \"\"\"\n",
        "    Plots bar charts and line plots displaying the relative composition\n",
        "    of a categorical target variable in each cluster \n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the dataset\n",
        "        target: the target variable to plot compositions of for each cluster\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
        "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
        "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "    print(f\"Clusters distribution across {target} levels\")\n",
        "    fig = px.bar(df_bar_plot, x='Clusters', y='Count', color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array', tickvals=df['Clusters'].unique()))\n",
        "    fig.show()\n",
        "\n",
        "    df_relative = (df\n",
        "                   .groupby([\"Clusters\", target])\n",
        "                   .size()\n",
        "                   .groupby(level=0)\n",
        "                   .apply(lambda x:  100*x / x.sum())\n",
        "                   .reset_index()\n",
        "                   .sort_values(by=['Clusters'])\n",
        "                   )\n",
        "    df_relative.columns = ['Clusters', target, 'Relative Percentage (%)']\n",
        "\n",
        "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)', color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array', tickvals=df['Clusters'].unique()))\n",
        "    fig.update_traces(mode='markers+lines')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "print(df_cluster_profile.shape)\n",
        "df_cluster_profile.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want also to analyse edibility levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_edible = pd.read_csv(\"outputs/datasets/collection/mushrooms.csv\").filter(['edible'])\n",
        "df_edible['edible'] = df_edible['edible'].astype('object')\n",
        "df_edible.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster profile based on the best features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We produce a profile of the proportion of categories that make up the various clusters, for the variables that were determined to function the best for the Predict Cluster model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_edible], axis=1))\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster distribution across edible levels & Relative Percentage of edible in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_edible=  df_edible.copy()\n",
        "df_cluster_vs_edible['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_edible, target='edible')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some insight can be derived about each mushrooms in each cluster from the above:\n",
        "\n",
        "- Mushrooms in cluster 0:\n",
        "    - are either in populations of several or solitary\n",
        "    - typically have broad gill sizes\n",
        "    - typically have a bulbous stalk root\n",
        "    - typically have close gill spacing\n",
        "    - tend to have a stalk color below the ring of white, but can also have gray, or pink. \n",
        "    - have pendant stalk ring(s)\n",
        "    - are usually found in the woods, occasionally in urban or grass habitats.\n",
        "    - **are usually edible (86% of cluster), but may occasionally be poisonous (14% of cluster)**\n",
        "\n",
        "* Mushrooms in cluster 1:\n",
        "    - are either in populations of clustered or several\n",
        "    - have broad gill sizes\n",
        "    - have missing stalk roots\n",
        "    - have close gill spacing\n",
        "    - have a orange stalk color below the ring\n",
        "    - have pendant stalk ring(s)\n",
        "    - are found in leaves habitats\n",
        "    - **are always edible (100% of cluster)**\n",
        "\n",
        "- Mushrooms in cluster 2:\n",
        "    - tend to be in populations of scattered, but are also observed in numerous and several\n",
        "    - usually have broad gill sizes, but may be narrow\n",
        "    - tend to have a club stalk root, but also may be equal, or rooted\n",
        "    - have a close gill spacing\n",
        "    - have a white stalk color below the ring\n",
        "    - have pendant stalk ring(s) \n",
        "    - tend to be found in grasses habitat, but may also be found in meadows, or urban\n",
        "    - **are usually edible (74%), but may be poisonous (26% of cluster)**\n",
        "\n",
        "* Mushrooms in cluster 3:\n",
        "    - are either in populations of several or solitary\n",
        "    - typically have broad gill sizes\n",
        "    - typically have a bulbous stalk root\n",
        "    - have close gill-spacing\n",
        "    - either have brown, buff, or pink stalk color below the ring\n",
        "    - typically have a large stalk ring\n",
        "    - are either found in woods, paths, or grasses habitats\n",
        "    - **are almost always poisonous (99% of cluster)**\n",
        "\n",
        "- Mushrooms in cluster 4:\n",
        "    - are typically in populations of several\n",
        "    - have narrow gill sizes\n",
        "    - typically have a missing stalk root\n",
        "    - typically have a close gill spacing\n",
        "    - either have a white or pink stalk color below the stalk ring\n",
        "    - typically have an evanescent stalk ring\n",
        "    - are either found in woods, leaves, or paths habitats \n",
        "    - **are almost always poisonous (98% of cluster)**\n",
        "\n",
        "* Mushrooms in cluster 5:\n",
        "    - tend to be found in populations of either clustered, numerous, or scattered\n",
        "    - have broad gill sizes\n",
        "    - usually have a missing stalk root, but may also have a bulbous stalk root\n",
        "    - either have close or crowded gill spacing\n",
        "    - usually have a white stalk color below the stalk ring, but may be red\n",
        "    - mostly have a pendant stalk ring(s), but may have an evanescent\n",
        "    - **are usually edible (88% of cluster), but may occasionally be poisonous (12% of cluster)**\n",
        "\n",
        "- Mushrooms in cluster 6:\n",
        "    - are usually found in either populations of scattered or abundant, but may be found in several\n",
        "    - usually have broad gill sizes, but may have narrow\n",
        "    - usually have an equal stalk root, but may have bulbous\n",
        "    - most usually have crowded gill spacings, but may have close\n",
        "    - typically have white a white stalk color below the stalk ring\n",
        "    - usually have evanescent stalk ring(s), but may have pendant\n",
        "    - are usually found in grasses habitat, but may be found in woods or leaves\n",
        "    - **are usually edible (89% of cluster), but may occasionally be poisonous (11% of cluster)**\n",
        "\n",
        "In terms of the business case, cluster 1 is most useful to identify, due the near absolute certainty such mushrooms are edible. Clusters 3 and 4 are also useful, in that they give a useful heurestic to avoid picking almost half the mushrooms that make up the dataset, and hence what is likely half of the mushrooms the company potentially picks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Classify Pipeline\n",
        "* Cluster Train Set\n",
        "* Classify Train and Test Sets\n",
        "* Feature Importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will be available to cluster the historical training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classify pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will be available to classify live data on the dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_clf_cluster, filename=f\"{file_path}/classify_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Set - Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and Test Sets - Classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train_clf.csv\", index=False)\n",
        "X_test.to_csv(f\"{file_path}/X_test_clf.csv\", index=False)\n",
        "y_train.to_csv(f\"{file_path}/y_train_clf.csv\", index=False)\n",
        "y_test.to_csv(f\"{file_path}/y_test_clf.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(7,5))\n",
        "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
        "fig.fit(df_analysis)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

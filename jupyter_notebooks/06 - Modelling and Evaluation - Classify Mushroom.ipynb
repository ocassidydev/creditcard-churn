{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar data\n",
        "* Understand the profile for each cluster\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/mushrooms.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Most important features to define a cluster plot\n",
        "* Clusters Profile Description\n",
        "* Cluster Silhouette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/collection/mushrooms.csv\").drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ML Cluster Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Feature Engineering\n",
        "from category_encoders import TargetEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ML algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.6, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "    ])\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df, df['class'])\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "n_components = 17\n",
        "\n",
        "def pca_components_analysis(df_pca, n_components):\n",
        "    pca = PCA(n_components=n_components).fit(df_pca)\n",
        "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
        "\n",
        "    ComponentsList = [\"Component \" + str(number)\n",
        "                      for number in range(n_components)]\n",
        "    dfExplVarRatio = pd.DataFrame(\n",
        "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "        index=ComponentsList,\n",
        "        columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
        "    )\n",
        "\n",
        "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(np.arange(0, 110, 10))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_components_analysis(df_pca=df_pca,n_components=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.6, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        # we update n_components to 6\n",
        "        (\"PCA\", PCA(n_components=6, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elbow Method and Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df, df['class'])\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,17)) # 17 is not inclusive, it will plot until 16\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "# 9 is not inclusive, it will stop at 8\n",
        "n_cluster_start, n_cluster_stop = 2, 9\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
        "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                      colors='yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    plt.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.6, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=6, random_state=0)),\n",
        "\n",
        "        # we update n_clusters to 4\n",
        "        (\"model\", KMeans(n_clusters=4, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy of data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X, X['class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add a column `Clusters` (with the cluster pipeline predictions) to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],\n",
        "                hue=X['Clusters'], palette='Set1', alpha=0.6)\n",
        "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], y=pipeline_cluster['model'].cluster_centers_[:, 1],\n",
        "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
        "plt.xlabel(\"PCA Component 0\")\n",
        "plt.ylabel(\"PCA Component 1\")\n",
        "plt.title(\"PCA Components colored by Clusters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save the cluster predictions from this pipeline to use in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a classifier, where the target is cluster predictions and features remaining variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We copy `X` to a DataFrame `dl_clf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create classifier pipeline steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithm\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.6, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"feat_selection\", SelectFromModel(\n",
        "            AdaBoostClassifier(random_state=0))),\n",
        "\n",
        "        (\"model\", AdaBoostClassifier(random_state=0)),\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit classifier to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assess the most important Features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after data cleaning and feature engineering, the feature space changes\n",
        "\n",
        "# how many data cleaning and feature engineering steps does your pipeline have?\n",
        "data_cleaning_feat_eng_steps = 2\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\n",
        ")].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# reassign best features in importance order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will store the best_features to use at a later stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load function that plots a table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DescriptionAllClusters(df, decimal_points=3):\n",
        "\n",
        "    DescriptionAllClusters = pd.DataFrame(\n",
        "        columns=df.drop(['Clusters'], axis=1).columns)\n",
        "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
        "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "        EDA_ClusterSubset = df.query(\n",
        "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
        "        ClusterDescription = Clusters_IndividualDescription(\n",
        "            EDA_ClusterSubset, cluster, decimal_points)\n",
        "        DescriptionAllClusters = DescriptionAllClusters.append(\n",
        "            ClusterDescription)\n",
        "\n",
        "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
        "    return DescriptionAllClusters\n",
        "\n",
        "\n",
        "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
        "\n",
        "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "    # for a given cluster, iterate over all columns\n",
        "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
        "    # That will show the range for the most common values for the numerical variable\n",
        "    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\n",
        "    # That will show the most common levels for the category\n",
        "\n",
        "    for col in EDA_Cluster.columns:\n",
        "\n",
        "        try:  # eventually a given cluster will have only missing data for a given variable\n",
        "\n",
        "            if EDA_Cluster[col].dtypes == 'object':\n",
        "\n",
        "                top_frequencies = EDA_Cluster.dropna(\n",
        "                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "                Description = ''\n",
        "\n",
        "                for x in range(len(top_frequencies)):\n",
        "                    freq = top_frequencies.iloc[x]\n",
        "                    category = top_frequencies.index[x][0]\n",
        "                    CategoryPercentage = int(round(freq*100, 0))\n",
        "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
        "                    Description = Description + statement\n",
        "\n",
        "                ClustersDescription.at[0, col] = Description[:-2]\n",
        "\n",
        "            elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
        "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
        "                Description = f\"{Q1} -- {Q3}\"\n",
        "                ClustersDescription.at[0, col] = Description\n",
        "\n",
        "        except Exception as e:\n",
        "            ClustersDescription.at[0, col] = 'Not available'\n",
        "            print(\n",
        "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "\n",
        "    ClustersDescription['Cluster'] = str(cluster)\n",
        "\n",
        "    return ClustersDescription"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "\n",
        "def cluster_distribution_per_variable(df, target):\n",
        "    \"\"\"\n",
        "    The data should have 2 variables, the cluster predictions and\n",
        "    the variable you want to analyze with, in this case we call \"target\".\n",
        "    We use plotly express to create 2 plots:\n",
        "    Cluster distribution across the target.\n",
        "    Relative presence of the target level in each cluster.\n",
        "    \"\"\"\n",
        "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
        "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
        "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "    print(f\"Clusters distribution across {target} levels\")\n",
        "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
        "                 color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.show()\n",
        "\n",
        "    df_relative = (df\n",
        "                   .groupby([\"Clusters\", target])\n",
        "                   .size()\n",
        "                   .groupby(level=0)\n",
        "                   .apply(lambda x:  100*x / x.sum())\n",
        "                   .reset_index()\n",
        "                   .sort_values(by=['Clusters'])\n",
        "                   )\n",
        "    df_relative.columns = ['Clusters', target, 'Relative Percentage (%)']\n",
        "\n",
        "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
        "                  color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.update_traces(mode='markers+lines')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "print(df_cluster_profile.shape)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want also to analyse edibility levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_edible = pd.read_csv(\"outputs/datasets/collection/mushrooms.csv\").filter(['class'])\n",
        "df_edible['class'] = df_edible['class'].astype('object')\n",
        "df_edible.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster profile based on the best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_edible], axis=1), decimal_points=0)\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster distribution across edible levels & Relative Percentage of edible in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_edible=  df_edible.copy()\n",
        "df_cluster_vs_edible['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_edible, target='class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit New Cluster Pipeline with most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to reduce feature space, we will study the trade-off between the previous Cluster Pipeline (fitted with all variables) and Pipeline using the variables that are most important to define the clusters from the previous pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define trade-off and metrics to compare new and previous Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate this trade-off we will\n",
        "\n",
        "1. Conduct a elbow method and silhouette analysis and check if the same number of clusters is suggested\n",
        "2. Fit new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline\n",
        "3. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar to the previous pipeline\n",
        "4. Check if the most important features for the classifier are the same from the previous pipeline\n",
        "5. Compare if the cluster profile from both pipelines are \"equivalent\"\n",
        "\n",
        "If we are happy to say yes for them, we can use a cluster pipeline using the features that best define the clusters from previous pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subset data with the most relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reduced = df.filter(best_features_pipeline_all_variables)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rewrite Cluster Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        # we update the pipeline, considering only the most important variables from the previous pipeline\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "\n",
        "        # it doesn't need SmartCorrelation\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        # No PCA step needed, since we know which features to consider\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=4, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Elbow Method and Silhouette analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df_reduced, df['class'])\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "n_cluster_start, n_cluster_stop = 2, 9\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
        "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                      colors='yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    plt.show()\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit New Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set X as training set for the cluster. It's a copy of df_reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_reduced.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X, df['class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We add column `Clusters` (with cluster pipeline predictions) to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare current cluster predictions to previous cluster predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just fitted a new cluster pipeline, and wish to compare if its predictions are \"equivalent\" to the previous cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the predictions from the previous cluster pipeline - trained with all variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And these are the predictions from current cluster pipeline (trained with `['gill-size', 'stalk-surface-below-ring', 'gill-spacing', 'population', 'bruises', 'stalk-root']`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_predictions_with_best_features = X['Clusters'] \n",
        "cluster_predictions_with_best_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use a confusion matrix to evaluate if the predictions of both pipelines are \"equivalent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(cluster_predictions_with_all_variables, cluster_predictions_with_best_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ordering of the middle two clusters has been reversed, but otherwise these two pipelines are reasonably equivalent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit a classifier, where the target is cluster predictions and features remaining variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rewrite pipeline to explain clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineClf2ExplainClusters():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"TargetEncoder\", TargetEncoder()),\n",
        "\n",
        "        # it doesn't need SmartCorrelation\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        # we don't consider feature selection step, since we know which features to consider\n",
        "\n",
        "        (\"model\", AdaBoostClassifier(random_state=0)),\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit a classifier, where the target is cluster labels and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create and fit a classifier pipeline to learn the feature importance when defining a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assess Most Important Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# since we don't have feature selection step in this pipeline, best_features is Xtrain columns\n",
        "best_features = X_train.columns.to_list()\n",
        "\n",
        "# create a DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': best_features,\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a DataFrame that contains the best features and Clusters Predictions: we want to analyse the patterns for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want also to analyse edibility levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_edible = pd.read_csv(\"outputs/datasets/collection/mushrooms.csv\").filter(['class'])\n",
        "df_edible['class'] = df_edible['class'].astype('object')\n",
        "df_edible.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cluster profile on most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(df= pd.concat([df_cluster_profile,df_edible], axis=1), decimal_points=0)\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_edible=  df_edible.copy()\n",
        "df_cluster_vs_edible['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_edible, target='class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Which pipeline to deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature Importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_reduced.shape)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(7,5))\n",
        "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
        "fig.fit(df_analysis)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

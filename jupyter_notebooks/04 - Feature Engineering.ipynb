{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Engineer features for classification and cluster models\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/cleaned/TrainSetCleaned.csv\n",
        "* outputs/datasets/cleaned/TestSetCleaned.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate a list of variables to engineer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Need to change working directory from the current **jupyter_notebooks** folder to the parent folder in order to access the whole project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_path = \"outputs/datasets/cleaned/TestSetCleaned.csv\"\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running generating `ProfileReport` on the test set to investigate potential transformations that may be made"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Correlation and PPS Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No expected change compared to data cleaning notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will use altered form of the custom function from the feature engineering lessons, as there is no available numerical feature engineering for this project and the only feature engineering will be done via categorical encoders. Three encoders will be compared, `OneHotEncoder`, `OrdinalEncoder`, and `TargetEncoder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine.encoding import OneHotEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    Used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    Calls transformer_evaluation on the transformed data to assess the \n",
        "    distribution of transformed data\n",
        "    Taken from Walkthrough Project 02 - Churnometer, altered for the feature\n",
        "    engineering in this project\n",
        "\n",
        "    Args:\n",
        "        df: the DataFrame containing the dataset\n",
        "        analysis_type: a string that declares which encoder should be applied\n",
        "    \n",
        "    Returns:\n",
        "        df_feat_eng: a DataFrame that contains both the original dataset variables\n",
        "                    and their respective feature engineered variables for comparison\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['target_encoder', 'ordinal_encoder', 'one_hot_encoder']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        if column == 'edible':\n",
        "            continue\n",
        "        else:\n",
        "            df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "            for method in list_column_transformers:\n",
        "                df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "            df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "                analysis_type, df_feat_eng, column, df['edible'])\n",
        "\n",
        "            transformer_evaluation(\n",
        "                column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" \n",
        "    Checks analysis type against list of permitted type, throws error if not allowed\n",
        "    Taken from Walkthrough Project 02 - Churnometer\n",
        "\n",
        "    Args:\n",
        "        analysis_type: a string declaring the desired feature engineering method to use\n",
        "        allowed_types: a list of strings of the feature engineering types the function \n",
        "                        supports\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    \"\"\"\n",
        "    Checks the dataset for any null values so they are not passed into the function, \n",
        "    throws error if there is\n",
        "    Taken from Walkthrough Project 02 - Churnometer\n",
        "\n",
        "    Args:\n",
        "        df: the DataFrame containing the dataset\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" \n",
        "    Set suffix columns according to analysis_type\n",
        "    Taken from Walkthrough Project 02 - Churnometer\n",
        "    \n",
        "    Args:\n",
        "        analysis_type: A string declaring which feature engineering \n",
        "                    method to use\n",
        "    \n",
        "    Returns:\n",
        "        list_column_transformers: A list declaring which feature \n",
        "                                engineering method to use\n",
        "    \"\"\"\n",
        "    if analysis_type == 'target_encoder':\n",
        "        list_column_transformers = [\"target_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'one_hot_encoder':\n",
        "        list_column_transformers = ['one_hot_encoder']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column, target):\n",
        "    \"\"\"\n",
        "    Applies the transformers to a variable column by calling the \n",
        "    appropriate encoder function\n",
        "    Taken from Walkthrough Project 02 - Churnometer - altered to support \n",
        "    different feature engineering methods\n",
        "\n",
        "    Args:\n",
        "        analysis_type: the feature engineering method to apply\n",
        "        df_feat_eng: the DataFrame containing the dataset variable to be \n",
        "                    transformed\n",
        "        column: the dataset variable to transform\n",
        "        target: the target variable series from the dataset DataFrame, \n",
        "                needed for the target encoder\n",
        "\n",
        "    Returns:\n",
        "        df_feat_eng: a DataFrame that has the original column and it's \n",
        "                    feature engineered column(s) appended to it\n",
        "        list_applied_tranformers: a list of the different transformations \n",
        "                                applied for the column, for use in \n",
        "                                transformer_evaluation\n",
        "    \"\"\"\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'target_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_TargetEncoder(\n",
        "            df_feat_eng, column, target)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OrdinalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'one_hot_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OneHotEncoder(\n",
        "            df_feat_eng, column)\n",
        "    \n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    \"\"\" \n",
        "    Assesses how the transformations alter feature distribution for a variable\n",
        "    Taken from Walkthrough Project 02 - Churnometer - altered to support \n",
        "    different feature engineering methods\n",
        "\n",
        "    Args:\n",
        "        column: the variable to assess the effect of the transformation on\n",
        "        list_applied_transformers: the columns in the Dataframe containing the\n",
        "                                    transformed variables\n",
        "        \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "        if col == column:\n",
        "            DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "        else:\n",
        "            if analysis_type == \"one_hot_encoder\":\n",
        "                for sub_col in [i for i in list(df_feat_eng.drop([column], axis=1).columns) if \"one_hot_encoder\" in i and column in i]:\n",
        "                    DiagnosticPlots_Numerical(df_feat_eng, sub_col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    \"\"\"\n",
        "    Provides a diagnostic plot for the distribution of a categorical variable\n",
        "    Taken from Walkthrough Project 02 - Churnometer \n",
        "\n",
        "    Args:\n",
        "        df_feat_eng: DataFrame containing the variable to plot distribution of\n",
        "        col: the variable to plot\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    \"\"\"\n",
        "    Provides the diagnotic plots for the distribution of an encoded numerical variable\n",
        "    Taken from Walkthrough Project 02 - Churnometer \n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the transformed variable to plot diagnostic plots of\n",
        "        variable: the variable to plot diagnostic plots of \n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_TargetEncoder(df_feat_eng, column, target):\n",
        "    \"\"\"\n",
        "    Applies target encoder to dataset column\n",
        "\n",
        "    Args:\n",
        "        df_feat_eng: DataFrame containing the variable to apply target encoding to\n",
        "        column: the variable to perform target encoding on\n",
        "        target: the ordered data series of target values for encoding\n",
        "\n",
        "    Returns:\n",
        "        df_feat_eng: DataFrame with the target encoded variable appended\n",
        "        list_methods_worked: a list of the encoded variables produced\n",
        "    \"\"\"\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = TargetEncoder()\n",
        "        df_feat_eng[f\"{column}_target_encoder\"] = encoder.fit_transform(df_feat_eng[f\"{column}_target_encoder\"].to_frame(), target)\n",
        "        list_methods_worked.append(f\"{column}_target_encoder\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_target_encoder\"], axis=1, inplace=True)\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OrdinalEncoder(df_feat_eng, column):\n",
        "    \"\"\"\n",
        "    Applies ordinal encoder to dataset column\n",
        "\n",
        "    Args:\n",
        "        df_feat_eng: DataFrame containing the variable to apply ordinal encoding to\n",
        "        column: the variable to perform ordinal encoding on\n",
        "\n",
        "    Returns:\n",
        "        df_feat_eng: DataFrame with the ordinal encoded variable appended\n",
        "        list_methods_worked: a list of the encoded variables produced\n",
        "    \"\"\"\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(categories=[list(df_feat_eng[column].unique())])\n",
        "        df_feat_eng[f\"{column}_ordinal_encoder\"] = encoder.fit_transform(df_feat_eng[f\"{column}_ordinal_encoder\"].to_frame())\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OneHotEncoder(df_feat_eng, column):\n",
        "    \"\"\"\n",
        "    Applies OneHotEncoder encoder to dataset column\n",
        "\n",
        "    Args:\n",
        "        df_feat_eng: DataFrame containing the variable to apply ordinal encoding to\n",
        "        column: the variable to perform ordinal encoding on\n",
        "\n",
        "    Returns:\n",
        "        df_feat_eng: DataFrame with the ordinal encoded variable appended\n",
        "        list_methods_worked: a list of the encoded variables produced\n",
        "    \"\"\"\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OneHotEncoder(variables=[f\"{column}_one_hot_encoder\"], drop_last=False)\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_one_hot_encoder\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_one_hot_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dealing with Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Encoding - Ordinal Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering = list(TrainSet.columns)\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create separate dataframe to store variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create engineered variables by applying the transformations, assess variable distribution and select the most suitable method for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As discussed previously in the Edibility Study notebook, a downside to the use of ordinal encoders is that categories are arbitrarily ordered. For a variable with two possible categories, this is unimportant, as the swapped numerical ordering of the categories won't have an effect on said number's correlation to the target variable, only whether the variable has a negative or positive correlation to the target (which is physically meaningless for categorical variables). However, in the case of categorical variables with three or more  (e.g. the `odor` variable, which has 9 distinct categories), ordering becomes relevant, and has an outsized effect on an encoded variables correlation to the target. This will adversely effect the performance of any model created using the data in an arbitrary fashion. Therefore this encoder will not be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Encoding - One Hot Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create separate dataframe to store variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create engineered variables by applying the transformations, assess variable distribution and select the most suitable method for each variable. (Warning - this code cell produces a huge output, may take upwards of a minute to run)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='one_hot_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen from the above output, `OneHotEncoder` applied to this dataset generates a massive number of variables, some of them being entirely skewed due to the rare occurence/common occurence of certain categories compared to others. It is unlikely this encoder will produce well-performing models, as many of the variables effectively have one unique value, and for those that have a better distribution there are too many variables, producing a situation of ['Curse of Dimensionality'](https://en.wikipedia.org/wiki/Curse_of_dimensionality), where there are so many variables that model performance deteriorates rather than improves. As such this encoder is not appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Encoding - Target Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create separate dataframe to store variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create engineered variables by applying the transformations, assess variable distribution and select the most suitable method for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='target_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen in the above output, the encoder succesfully transforms the variables into numerical values, which in most cases appear to have good distributions per their histograms, QQ plots, and boxplots. This encoding method neither has the disadvantage of arbitrary numerical ordering as with ordinal encoding nor the Curse of Dimensionality as with one hot encoding.\n",
        "* For all variables, target encoding will be used, with the target being taken from the TrainSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = TargetEncoder()\n",
        "TrainSet = encoder.fit_transform(TrainSet, TrainSet['class'])\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "print(\"* Categorical encoding - target transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create separate DataFrame with variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create engineered variables applying the transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Will add the following transformations to the ML Pipeline for feature engineering:\n",
        "\n",
        "* Target categorical encoding: `['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape',  'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']` with respect to `edible` as the target\n",
        "* Smart Correlation Selection: `['bruises', 'odor', 'gill-attachment', 'gill-color', 'stalk-surface-below-ring', 'spore-print-color']`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
